{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdf748b",
   "metadata": {},
   "source": [
    "# StellarRegGAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e5564",
   "metadata": {},
   "source": [
    "# Stellar Parameter Regression with MLP-ResNet\n",
    "\n",
    "We construct a multitask regressor that maps high-resolution stellar spectra to effective temperature (Teff), metallicity ([Fe/H]), and surface gravity (log g). The notebook lays out an end-to-end workflow that can be reproduced or adapted for collaborative astrophysical modeling projects.\n",
    "\n",
    "The narrative walks through preparing the data products, engineering spectroscopy-aware augmentations, configuring a residual multilayer perceptron, and interpreting the resulting predictions with diagnostics so the full methodology can be shared confidently in a public repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d880a6d",
   "metadata": {
    "id": "9d880a6d"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Standard library\n",
    "# ===============================\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple, Literal  # type hints\n",
    "\n",
    "# ===============================\n",
    "# Third-party libraries\n",
    "# ===============================\n",
    "import numpy as np  # numerical computing\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "\n",
    "from scipy.stats import gaussian_kde, norm, multivariate_normal  # statistics & densities\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score  # evaluation metrics\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler  # feature scaling\n",
    "from sklearn.decomposition import PCA  # dimensionality reduction\n",
    "\n",
    "import tensorflow as tf  # deep learning backend\n",
    "from keras import layers, Model, regularizers  # Keras layers, base Model, regularization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # training utilities\n",
    "\n",
    "import keras_tuner as kt  # hyperparameter tuning\n",
    "\n",
    "import joblib  # model/pipeline persistence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc383eca",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The experiments rely on pre-generated NumPy `.npy` tensors stored inside the `datasets/` directory.\n",
    "\n",
    "- `X_raw_*` arrays hold continuum-normalized flux values sampled on a common wavelength grid; each row represents a single stellar spectrum and each column corresponds to a wavelength bin.\n",
    "- `y_raw_*` arrays store the associated stellar labels stacked as `[Teff, [Fe/H], log g]` in their native physical units (Kelvin and dex).\n",
    "\n",
    "Place the training and test splits in `datasets/` before running the notebook. If you generate alternative calibrations or splits, update the filenames in the next cell to point to the new assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae320da",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('D:\\IC\\programas_IC_organizados\\problema_2\\datasets')\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Dataset directory 'datasets' was not found. Place the required .npy files in this folder.\"\n",
    "    )\n",
    "\n",
    "X_raw_train = np.load(DATA_DIR / 'X_raw_resampled_filtered_train.npy')\n",
    "y_raw_train = np.load(DATA_DIR / 'y_raw_loaded_filtered_train.npy')\n",
    "\n",
    "X_raw_test = np.load(DATA_DIR / 'X_raw_resampled_filtered_test.npy')\n",
    "y_raw_test = np.load(DATA_DIR / 'y_raw_loaded_filtered_test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YEOKfxIfDDMm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEOKfxIfDDMm",
    "outputId": "86e4b034-15a8-4587-a484-f8b2f3a5c4fa"
   },
   "outputs": [],
   "source": [
    "print(f'X_raw_train shape: {X_raw_train.shape}')\n",
    "print(f'y_raw_train shape: {y_raw_train.shape}')\n",
    "print(f'X_raw_test shape: {X_raw_test.shape}')\n",
    "print(f'y_raw_test shape: {y_raw_test.shape}')\n",
    "\n",
    "print(f'Teff range in y_raw_train: {y_raw_train[:, 0].min():.2f} - {y_raw_train[:, 0].max():.2f}')\n",
    "print(f'[Fe/H] range in y_raw_train: {y_raw_train[:, 1].min():.2f} - {y_raw_train[:, 1].max():.2f}')\n",
    "print(f'log g range in y_raw_train: {y_raw_train[:, 2].min():.2f} - {y_raw_train[:, 2].max():.2f}')\n",
    "print(f'Teff range in y_raw_test: {y_raw_test[:, 0].min():.2f} - {y_raw_test[:, 0].max():.2f}')\n",
    "print(f'[Fe/H] range in y_raw_test: {y_raw_test[:, 1].min():.2f} - {y_raw_test[:, 1].max():.2f}')\n",
    "print(f'log g range in y_raw_test: {y_raw_test[:, 2].min():.2f} - {y_raw_test[:, 2].max():.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe10c4",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V2C4XFSHGa9o",
   "metadata": {
    "id": "V2C4XFSHGa9o"
   },
   "outputs": [],
   "source": [
    "def filter_stellar_sample(X, y, plot_hr_diagram=True, return_indices=False):\n",
    "    \"\"\"\n",
    "    Apply broad physical sanity checks and IQR-based outlier removal to labels and fluxes.\n",
    "\n",
    "    This keeps stars from different evolutionary stages while discarding obvious outliers.\n",
    "    \"\"\"\n",
    "    teff = y[:, 0]\n",
    "    feh = y[:, 1]\n",
    "    logg = y[:, 2]\n",
    "\n",
    "    print(\"Applying physical sanity filters...\")\n",
    "    print(f\"Initial sample size: {len(y)}\")\n",
    "\n",
    "    total = len(y)\n",
    "    mask = np.ones(total, dtype=bool)\n",
    "\n",
    "    temp_mask = (teff >= 2500) & (teff <= 10000)\n",
    "    mask &= temp_mask\n",
    "    print(f\"After temperature sanity check (2500 <= Teff <= 10000 K): {np.sum(mask)}\")\n",
    "\n",
    "    logg_mask = (logg >= 0.0) & (logg <= 6.0)\n",
    "    mask &= logg_mask\n",
    "    print(f\"After surface gravity sanity check (0 <= log g <= 6): {np.sum(mask)}\")\n",
    "\n",
    "    metallicity_mask = (feh >= -5.0) & (feh <= 1.0)\n",
    "    mask &= metallicity_mask\n",
    "    print(f\"After metallicity sanity check (-5.0 <= [Fe/H] <= +1.0): {np.sum(mask)}\")\n",
    "\n",
    "    def iqr_outlier_mask(values, base_mask, multiplier=1.5):\n",
    "        subset = values[base_mask]\n",
    "        if subset.size < 2:\n",
    "            return np.ones_like(values, dtype=bool)\n",
    "        q1 = np.percentile(subset, 25)\n",
    "        q3 = np.percentile(subset, 75)\n",
    "        iqr = q3 - q1\n",
    "        if iqr == 0:\n",
    "            return np.ones_like(values, dtype=bool)\n",
    "        lower = q1 - multiplier * iqr\n",
    "        upper = q3 + multiplier * iqr\n",
    "        return (values >= lower) & (values <= upper)\n",
    "\n",
    "    teff_outlier_mask = iqr_outlier_mask(teff, mask)\n",
    "    feh_outlier_mask = iqr_outlier_mask(feh, mask)\n",
    "    logg_outlier_mask = iqr_outlier_mask(logg, mask)\n",
    "\n",
    "    mask &= teff_outlier_mask\n",
    "    print(f\"After IQR temperature outlier removal: {np.sum(mask)}\")\n",
    "    mask &= feh_outlier_mask\n",
    "    print(f\"After IQR metallicity outlier removal: {np.sum(mask)}\")\n",
    "    mask &= logg_outlier_mask\n",
    "    print(f\"After IQR gravity outlier removal: {np.sum(mask)}\")\n",
    "\n",
    "    def flux_iqr_mask(spectra, base_mask, multiplier=1.5, tol_frac=0.01):\n",
    "        subset = spectra[base_mask]\n",
    "        if subset.size == 0:\n",
    "            return np.ones(spectra.shape[0], dtype=bool)\n",
    "        q1 = np.percentile(subset, 25); q3 = np.percentile(subset, 75)\n",
    "        iqr = q3 - q1\n",
    "        if iqr == 0:\n",
    "            return np.ones(spectra.shape[0], dtype=bool)\n",
    "        lower, upper = q1 - multiplier*iqr, q3 + multiplier*iqr\n",
    "        within = (spectra >= lower) & (spectra <= upper)\n",
    "        # keep if at least (1 - tol_frac) pixels are within bounds\n",
    "        return (within.sum(axis=1) / spectra.shape[1]) >= (1 - tol_frac)\n",
    "\n",
    "\n",
    "    flux_outlier_mask = flux_iqr_mask(X, mask)\n",
    "    mask &= flux_outlier_mask\n",
    "    print(f\"After IQR flux outlier removal: {np.sum(mask)}\")\n",
    "\n",
    "    finite_mask = np.isfinite(X).all(axis=1) & np.isfinite(y).all(axis=1)\n",
    "    mask &= finite_mask\n",
    "    print(f\"After removing rows with non-finite values: {np.sum(mask)}\")\n",
    "\n",
    "    retained = int(np.sum(mask))\n",
    "    retention_rate = (retained / total * 100.0) if total else 0.0\n",
    "    print(f\"Final sample after quality filtering: {retained} stars\")\n",
    "    print(f\"Retention rate: {retention_rate:.1f}%\")\n",
    "\n",
    "    if plot_hr_diagram:\n",
    "        plot_hr_filtering(teff, logg, mask)\n",
    "\n",
    "    X_filtered = X[mask]\n",
    "    y_filtered = y[mask]\n",
    "\n",
    "    if return_indices:\n",
    "        indices = np.where(mask)[0]\n",
    "        return X_filtered, y_filtered, indices\n",
    "\n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "\n",
    "def plot_hr_filtering(teff, logg, keep_mask):\n",
    "    \"\"\"Plot HR diagram showing the filtering process.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot all stars\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(teff, logg, alpha=0.5, s=1, c='gray', label='All stars')\n",
    "    plt.xlabel('Effective Temperature (K)', fontsize=20)\n",
    "    plt.ylabel('log g (surface gravity)', fontsize=20)\n",
    "    plt.title('Original Sample', fontsize=24)\n",
    "    plt.gca().invert_xaxis()  # Hot stars on the left\n",
    "    plt.gca().invert_yaxis()  # High gravity at bottom\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot retained vs removed stars\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(teff[~keep_mask], logg[~keep_mask], alpha=0.3, s=1, c='red', label='Removed')\n",
    "    plt.scatter(teff[keep_mask], logg[keep_mask], alpha=0.7, s=2, c='blue', label='Retained')\n",
    "    plt.xlabel('Effective Temperature (K)', fontsize=20)\n",
    "    plt.ylabel('log g (surface gravity)', fontsize=20)\n",
    "    plt.title('After Quality Filtering', fontsize=24)\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def improved_preprocessing_pipeline(X_train_aug, X_test,\n",
    "                                   y_train_aug, y_test,\n",
    "                                   use_robust_scaling=True,\n",
    "                                   apply_physical_filters_train=True,\n",
    "                                   apply_physical_filters_test=False):\n",
    "    \"\"\"\n",
    "    Improved preprocessing pipeline with optional physical sanity filtering and robust scaling,\n",
    "    but without PCA (uses full normalized spectra).\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"IMPROVED PREPROCESSING PIPELINE (NO PCA)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 1. Apply physical sanity filters if requested\n",
    "    if apply_physical_filters_train:\n",
    "        print(\"\\n1. Applying physical sanity filters to training...\")\n",
    "        X_train_aug, y_train_aug = filter_stellar_sample(\n",
    "            X_train_aug, y_train_aug, plot_hr_diagram=True\n",
    "        )\n",
    "    if apply_physical_filters_test:\n",
    "        print(\"\\n1. Applying physical sanity filters to test...\")\n",
    "        X_test, y_test = filter_stellar_sample(\n",
    "            X_test, y_test, plot_hr_diagram=True\n",
    "        )\n",
    "\n",
    "    # 2. Spectral normalization (per spectrum)\n",
    "    print(\"\\n2. Normalizing spectra...\")\n",
    "\n",
    "    def normalize_spectra(X, eps=1e-8):\n",
    "        mu  = np.mean(X, axis=1, keepdims=True)\n",
    "        std = np.std(X, axis=1, keepdims=True)\n",
    "        return (X - mu) / (std + eps)\n",
    "\n",
    "    X_train_norm = normalize_spectra(X_train_aug)\n",
    "    X_test_norm = normalize_spectra(X_test)\n",
    "\n",
    "    # 3. Check for and handle NaN/inf values\n",
    "    print(\"\\n3. Checking for invalid values...\")\n",
    "    def clean_data(X, y, name):\n",
    "        X_mask = np.isfinite(X).all(axis=1)\n",
    "        y_mask = np.isfinite(y).all(axis=1)\n",
    "        combined_mask = X_mask & y_mask\n",
    "        if not combined_mask.all():\n",
    "            print(f\"   {name}: Removing {np.sum(~combined_mask)} samples with invalid values\")\n",
    "            X = X[combined_mask]\n",
    "            y = y[combined_mask]\n",
    "        else:\n",
    "            print(f\"   {name}: No invalid values found\")\n",
    "        return X, y\n",
    "\n",
    "    X_train_norm, y_train_aug = clean_data(X_train_norm, y_train_aug, \"Training\")\n",
    "    X_test_norm, y_test = clean_data(X_test_norm, y_test, \"Test\")\n",
    "\n",
    "    # 4. Target scaling (physics-aware)\n",
    "    print(\"\\n4. Scaling target parameters...\")\n",
    "    if use_robust_scaling:\n",
    "        scalers_y = [RobustScaler() for _ in range(y_train_aug.shape[1])]\n",
    "        print(\"   Using RobustScaler (better for outliers)\")\n",
    "    else:\n",
    "        scalers_y = [StandardScaler() for _ in range(y_train_aug.shape[1])]\n",
    "        print(\"   Using StandardScaler\")\n",
    "\n",
    "    param_names = ['Teff (K)', '[Fe/H] (dex)', 'log g (dex)']\n",
    "\n",
    "    y_train_scaled = np.zeros_like(y_train_aug, dtype=float)\n",
    "    y_test_scaled = np.zeros_like(y_test, dtype=float)\n",
    "\n",
    "    for i in range(y_train_aug.shape[1]):\n",
    "        scalers_y[i].fit(y_train_aug[:, i].reshape(-1, 1))\n",
    "        y_train_scaled[:, i] = scalers_y[i].transform(y_train_aug[:, i].reshape(-1, 1)).flatten()\n",
    "        y_test_scaled[:, i] = scalers_y[i].transform(y_test[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "        print(f\"   {param_names[i]}:\")\n",
    "        print(f\"     Original range: [{y_train_aug[:, i].min():.2f}, {y_train_aug[:, i].max():.2f}]\")\n",
    "        print(f\"     Scaled range: [{y_train_scaled[:, i].min():.2f}, {y_train_scaled[:, i].max():.2f}]\")\n",
    "\n",
    "    # 5. Final data summary\n",
    "    print(\"\\n5. Final data shapes:\")\n",
    "    print(f\"   X_train: {X_train_norm.shape}\")\n",
    "    print(f\"   X_test:  {X_test_norm.shape}\")\n",
    "    print(f\"   y_train: {y_train_scaled.shape}\")\n",
    "    print(f\"   y_test:  {y_test_scaled.shape}\")\n",
    "\n",
    "    processed_data = {\n",
    "        'X_train': X_train_norm,\n",
    "        'X_test': X_test_norm,\n",
    "        'y_train': y_train_scaled,\n",
    "        'y_test': y_test_scaled,\n",
    "        'y_train_original': y_train_aug,\n",
    "        'y_test_original': y_test\n",
    "    }\n",
    "\n",
    "    scalers = {'target_scalers': scalers_y}\n",
    "\n",
    "    return processed_data, scalers\n",
    "\n",
    "\n",
    "def run_complete_preprocessing(X_train_aug, X_test, y_train_aug, y_test):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline without PCA.\n",
    "    \"\"\"\n",
    "    processed_data, scalers = improved_preprocessing_pipeline(\n",
    "        X_train_aug, X_test,\n",
    "        y_train_aug, y_test,\n",
    "        use_robust_scaling=True,\n",
    "        apply_physical_filters_train=True,\n",
    "        apply_physical_filters_test=False\n",
    "    )\n",
    "\n",
    "    plot_parameter_distributions(\n",
    "        processed_data['y_train_original'],\n",
    "        processed_data['y_train']\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PREPROCESSING COMPLETE! (NO PCA)\")\n",
    "    print(\"=\"*50)\n",
    "    return processed_data, scalers\n",
    "\n",
    "\n",
    "def plot_parameter_distributions(y_original, y_scaled, param_names=['Teff (K)', '[Fe/H] (dex)', 'log g (dex)']):\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8), sharex=False)\n",
    "\n",
    "    # top: originals\n",
    "    for i in range(3):\n",
    "        ax = axes[0, i]\n",
    "        ax.hist(y_original[:, i], bins=50, alpha=0.8, edgecolor='black')\n",
    "        ax.set_title(f\"{param_names[i]} - Original\")\n",
    "        ax.set_xlabel(param_names[i].split()[0])\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # bottom: scaled (independent axes + correct labels)\n",
    "    for i, label in enumerate(['Scaled Teff', 'Scaled [Fe/H]', 'Scaled log g']):\n",
    "        ax = axes[1, i]\n",
    "        ax.hist(y_scaled[:, i], bins=50, alpha=0.8, edgecolor='black')\n",
    "        ax.set_title(f\"{label} - After Balanced Augmentation\")\n",
    "        ax.set_xlabel(label)\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_xlim(y_scaled[:, i].min(), y_scaled[:, i].max())  # ensure proper scale\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    fig.suptitle('Comparison of Parameter Distributions', fontsize=18)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def inverse_transform_predictions(y_scaled, scalers_y):\n",
    "    \"\"\"\n",
    "    Inverse transform scaled predictions back to original units\n",
    "    \"\"\"\n",
    "    y_original = np.zeros_like(y_scaled)\n",
    "\n",
    "    for i in range(y_scaled.shape[1]):\n",
    "        y_original[:, i] = scalers_y[i].inverse_transform(y_scaled[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "    return y_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vLmbJcaCD4lm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "vLmbJcaCD4lm",
    "outputId": "fae6e3da-3f1f-403c-c460-0e74ab8b85f2"
   },
   "outputs": [],
   "source": [
    "processed_data, scalers = run_complete_preprocessing(X_raw_train, X_raw_test, y_raw_train, y_raw_test)\n",
    "\n",
    "# Extract the final preprocessed data\n",
    "X_train_final = processed_data['X_train']\n",
    "X_test_final = processed_data['X_test']\n",
    "y_train_final = processed_data['y_train']\n",
    "y_test_final = processed_data['y_test']\n",
    "\n",
    "# Shuffle the training data\n",
    "perm = np.random.permutation(X_train_final.shape[0])\n",
    "X_train_final = X_train_final[perm]\n",
    "y_train_final = y_train_final[perm]\n",
    "\n",
    "X_val_final = X_train_final[-2000:]  # Last 2000 samples for validation\n",
    "y_val_final = y_train_final[-2000:]  # Last 2000 samples for validation\n",
    "\n",
    "\n",
    "X_train_final = X_train_final[:-2000]  # Remaining samples for training\n",
    "y_train_final = y_train_final[:-2000]  # Remaining samples for training\n",
    "\n",
    "print(f\"Final training set shape: X={X_train_final.shape}, y={y_train_final.shape}\")\n",
    "print(f\"Final validation set shape: X={X_val_final.shape}, y={y_val_final.shape}\")\n",
    "print(f\"Final test set shape: X={X_test_final.shape}, y={y_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train_final.astype('float32')\n",
    "X_val_final   = X_val_final.astype('float32')\n",
    "X_test_final  = X_test_final.astype('float32')\n",
    "y_train_final = y_train_final.astype('float32')\n",
    "y_val_final   = y_val_final.astype('float32')\n",
    "y_test_final  = y_test_final.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip all splits to the shared Teff, [Fe/H], and log g limits\n",
    "teff_min = max(y_train_final[:, 0].min(), y_val_final[:, 0].min())\n",
    "teff_max = min(y_train_final[:, 0].max(), y_val_final[:, 0].max())\n",
    "feH_min  = max(y_train_final[:, 1].min(), y_val_final[:, 1].min())\n",
    "feH_max  = min(y_train_final[:, 1].max(), y_val_final[:, 1].max())\n",
    "logg_min = max(y_train_final[:, 2].min(), y_val_final[:, 2].min())\n",
    "logg_max = min(y_train_final[:, 2].max(), y_val_final[:, 2].max())\n",
    "\n",
    "train_mask = (y_train_final[:, 0] >= teff_min) & (y_train_final[:, 0] <= teff_max) & \\\n",
    "             (y_train_final[:, 1] >= feH_min)  & (y_train_final[:, 1] <= feH_max)  & \\\n",
    "             (y_train_final[:, 2] >= logg_min) & (y_train_final[:, 2] <= logg_max)\n",
    "             \n",
    "val_mask = (y_val_final[:, 0] >= teff_min) & (y_val_final[:, 0] <= teff_max) & \\\n",
    "           (y_val_final[:, 1] >= feH_min)  & (y_val_final[:, 1] <= feH_max)  & \\\n",
    "           (y_val_final[:, 2] >= logg_min) & (y_val_final[:, 2] <= logg_max)    \n",
    "           \n",
    "test_mask = (y_test_final[:, 0] >= teff_min) & (y_test_final[:, 0] <= teff_max) & \\\n",
    "            (y_test_final[:, 1] >= feH_min)  & (y_test_final[:, 1] <= feH_max)  & \\\n",
    "            (y_test_final[:, 2] >= logg_min) & (y_test_final[:, 2] <= logg_max)\n",
    "\n",
    "X_train_final = X_train_final[train_mask]\n",
    "y_train_final = y_train_final[train_mask]\n",
    "\n",
    "X_val_final = X_val_final[val_mask]\n",
    "y_val_final = y_val_final[val_mask]\n",
    "\n",
    "X_test_final = X_test_final[test_mask]\n",
    "y_test_final = y_test_final[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d67c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After applying common limits:\")\n",
    "print(f\"Final training set shape: X={X_train_final.shape}, y={y_train_final.shape}\")\n",
    "print(f\"Final validation set shape: X={X_val_final.shape}, y={y_val_final.shape}\")\n",
    "print(f\"Final test set shape: X={X_test_final.shape}, y={y_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the Teff, [Fe/H], and log g distributions across splits\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(y_train_final[:, 0], bins=50, alpha=0.5, label='Train', color='blue', density=True)\n",
    "plt.hist(y_val_final[:, 0], bins=50, alpha=0.5, label='Validation', color='orange', density=True)\n",
    "plt.hist(y_test_final[:, 0], bins=50, alpha=0.5, label='Test', color='green', density=True)\n",
    "plt.xlabel('Scaled Teff', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.title('Distribution of Scaled Teff', fontsize=16)\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(y_train_final[:, 1], bins=50, alpha=0.5, label='Train', color='blue', density=True)\n",
    "plt.hist(y_val_final[:, 1], bins=50, alpha=0.5, label='Validation', color='orange', density=True)\n",
    "plt.hist(y_test_final[:, 1], bins=50, alpha=0.5, label='Test', color='green', density=True)\n",
    "plt.xlabel('Scaled [Fe/H]', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.title('Distribution of Scaled [Fe/H]', fontsize=16)\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(y_train_final[:, 2], bins=50, alpha=0.5, label='Train', color='blue', density=True)\n",
    "plt.hist(y_val_final[:, 2], bins=50, alpha=0.5, label='Validation', color='orange', density=True)\n",
    "plt.hist(y_test_final[:, 2], bins=50, alpha=0.5, label='Test', color='green', density=True)\n",
    "plt.xlabel('Scaled log g', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.title('Distribution of Scaled log g', fontsize=16)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect limits in the original physical scale\n",
    "y_train_original = inverse_transform_predictions(y_train_final, scalers['target_scalers'])\n",
    "print(f\"Limits in training set:\")\n",
    "print(f\"Teff: {y_train_original[:, 0].min()} - {y_train_original[:, 0].max()}\")\n",
    "print(f\"FeH:  {y_train_original[:, 1].min()} - {y_train_original[:, 1].max()}\")\n",
    "print(f\"logg: {y_train_original[:, 2].min()} - {y_train_original[:, 2].max()}\")\n",
    "print(f\"Limits in validation set:\")\n",
    "y_val_original = inverse_transform_predictions(y_val_final, scalers['target_scalers'])  \n",
    "print(f\"Teff: {y_val_original[:, 0].min()} - {y_val_original[:, 0].max()}\")\n",
    "print(f\"FeH:  {y_val_original[:, 1].min()} - {y_val_original[:, 1].max()}\")\n",
    "print(f\"logg: {y_val_original[:, 2].min()} - {y_val_original[:, 2].max()}\")\n",
    "print(f\"Limits in test set:\")   \n",
    "y_test_original = inverse_transform_predictions(y_test_final, scalers['target_scalers'])\n",
    "print(f\"Teff: {y_test_original[:, 0].min()} - {y_test_original[:, 0].max()}\")\n",
    "print(f\"FeH:  {y_test_original[:, 1].min()} - {y_test_original[:, 1].max()}\")\n",
    "print(f\"logg: {y_test_original[:, 2].min()} - {y_test_original[:, 2].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71192bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from the test set while leaving them in the training data\n",
    "def remove_duplicates_from_test(y_train, y_val, X_test, y_test):\n",
    "    \"\"\"Remove duplicate entries from the test set based on labels\"\"\"\n",
    "    train_val_labels = set(map(tuple, np.vstack((y_train, y_val))))\n",
    "    \n",
    "    mask = np.array([tuple(label) not in train_val_labels for label in y_test])\n",
    "    \n",
    "    num_duplicates = np.sum(~mask)\n",
    "    if num_duplicates > 0:\n",
    "        print(f\"Removing {num_duplicates} duplicate entries from the test set.\")\n",
    "    else:\n",
    "        print(\"No duplicate entries to remove from the test set.\")\n",
    "    \n",
    "    return X_test[mask], y_test[mask]\n",
    "\n",
    "X_test_final, y_test_final = remove_duplicates_from_test(y_train_final, y_val_final, X_test_final, y_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test set shape after removing duplicates: X={X_test_final.shape}, y={y_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e27243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_on_train_and_val(y_train, y_val):\n",
    "    \"\"\"Check for duplicate entries between training and validation sets\"\"\"\n",
    "    train_labels = set(map(tuple, y_train))\n",
    "    val_labels = set(map(tuple, y_val))\n",
    "    \n",
    "    duplicates = train_labels.intersection(val_labels)\n",
    "    \n",
    "    if duplicates:\n",
    "        print(f\"Found {len(duplicates)} duplicate entries between training and validation sets.\")\n",
    "    else:\n",
    "        print(\"No duplicate entries found between training and validation sets.\")\n",
    "        \n",
    "check_duplicates_on_train_and_val(y_train_final, y_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a5482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot normalized spectra ===\n",
    "\n",
    "# Plot several normalized spectra for inspection (three plots per row across two rows)\n",
    "for i in range(6):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X_train_final[i], label=f'Spectrum {i+1}')\n",
    "    plt.title('Examples of Normalized Spectra (Training)', fontsize=24)\n",
    "    plt.xlabel('Pixel (res-sampled wavelength)', fontsize=20)\n",
    "    plt.ylabel('Normalized Flux', fontsize=20)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yfLblqJyjZ6Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfLblqJyjZ6Z",
    "outputId": "37a07b5a-96d8-4c92-9263-d1e998828ead"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Primitive transforms -----------------------------------------------------\n",
    "def add_noise(spectrum: np.ndarray, noise_level: float = 0.05) -> np.ndarray:\n",
    "    \"\"\"Additive Gaussian noise proportional to signal std.\"\"\"\n",
    "    signal_std = float(np.nanstd(spectrum))\n",
    "    noise_std = signal_std * noise_level\n",
    "    noise = np.random.normal(0.0, noise_std, spectrum.shape)\n",
    "    return spectrum + noise\n",
    "\n",
    "def shift_spectrum(spectrum: np.ndarray, max_shift: int = 2) -> np.ndarray:\n",
    "    \"\"\"Small circular shift (pixels).\"\"\"\n",
    "    shift_amount = np.random.randint(-max_shift, max_shift + 1)\n",
    "    return np.roll(spectrum, shift_amount)\n",
    "\n",
    "def ripple_spectrum(\n",
    "    spectrum: np.ndarray,\n",
    "    amplitude: float = 0.02,\n",
    "    freq_range: Tuple[float, float] = (1.0, 5.0)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Low-frequency multiplicative ripple (continuum modulation).\"\"\"\n",
    "    length = len(spectrum)\n",
    "    freq = np.random.uniform(*freq_range)\n",
    "    phase = np.random.uniform(0.0, 2.0 * np.pi)\n",
    "    wave = 1.0 + amplitude * np.sin(np.linspace(0.0, freq * 2.0 * np.pi, length) + phase)\n",
    "    return spectrum * wave\n",
    "\n",
    "# --- Sampler weights for BALANCED augmentation --------------------------------\n",
    "def _balanced_sampling_weights(\n",
    "    y_train: np.ndarray,\n",
    "    eps: float = 1e-8,\n",
    "    target: Literal[\"mvn\", \"self-kde\"] = \"mvn\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute per-sample weights ~ p_target(y) / p_current(y).\n",
    "    target='mvn' uses N(mean, cov); target='self-kde' flattens density via 1/p_current.\n",
    "    \"\"\"\n",
    "    assert y_train.ndim == 2, \"y_train must be (n_samples, n_targets)\"\n",
    "    kde = gaussian_kde(y_train.T)\n",
    "    p_current = kde(y_train.T)  # shape: (n_samples,)\n",
    "\n",
    "    if target == \"mvn\":\n",
    "        mean_target = np.mean(y_train, axis=0)\n",
    "        cov_target = np.cov(y_train.T)\n",
    "        mvn = multivariate_normal(mean=mean_target, cov=cov_target, allow_singular=True)\n",
    "        p_target = mvn.pdf(y_train)\n",
    "        w = p_target / (p_current + eps)\n",
    "    else:\n",
    "        # Equalize by down-weighting dense regions only\n",
    "        w = 1.0 / (p_current + eps)\n",
    "\n",
    "    w = np.clip(w, a_min=0.0, a_max=np.finfo(np.float64).max)\n",
    "    w_sum = w.sum()\n",
    "    if not np.isfinite(w_sum) or w_sum <= 0.0:\n",
    "        # Fall back to uniform\n",
    "        w = np.ones_like(w) / len(w)\n",
    "    else:\n",
    "        w /= w_sum\n",
    "    return w\n",
    "\n",
    "# --- Main API -----------------------------------------------------------------\n",
    "def augment_data(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    augmentation_factor: float = 3.0,\n",
    "    mode: Literal[\"balanced\", \"uniform\", \"none\"] = \"balanced\",\n",
    "    balanced_target: Literal[\"mvn\", \"self-kde\"] = \"mvn\",\n",
    "    transform_p: Dict[str, float] = None,\n",
    "    transform_cfg: Dict[str, dict] = None,\n",
    "    rng_seed: Optional[int] = 42,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Configurable augmentation for 1D spectra.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : (N, L) float\n",
    "        Spectra matrix (N samples, L wavelengths).\n",
    "    y_train : (N, D) float\n",
    "        Targets matrix (D parameters).\n",
    "    augmentation_factor : float\n",
    "        Total size multiplier (e.g., 3.0 => produce ~2N synthetic + keep N original).\n",
    "        If <= 1.0 or mode=='none', returns inputs unchanged.\n",
    "    mode : {'balanced','uniform','none'}\n",
    "        'balanced' -> weighted resampling using KDE (and MVN target or self-KDE equalization).\n",
    "        'uniform'  -> uniform random resampling.\n",
    "        'none'     -> no augmentation.\n",
    "    balanced_target : {'mvn','self-kde'}\n",
    "        For 'balanced' mode: use MVN target or self-KDE flattening.\n",
    "    transform_p : dict\n",
    "        Per-transform application probabilities, e.g.:\n",
    "        {'noise':0.5, 'shift':0.5, 'ripple':0.5}\n",
    "    transform_cfg : dict\n",
    "        Per-transform hyperparameters, e.g.:\n",
    "        {'noise':{'noise_level':0.03}, 'shift':{'max_shift':2}, 'ripple':{'amplitude':0.01,'freq_range':(1,5)}}\n",
    "    rng_seed : int or None\n",
    "        Seed for numpy/random for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_out, y_out : arrays\n",
    "        Augmented (or original) datasets.\n",
    "    \"\"\"\n",
    "    if rng_seed is not None:\n",
    "        np.random.seed(rng_seed)\n",
    "        random.seed(rng_seed)\n",
    "\n",
    "    N = X_train.shape[0]\n",
    "    if augmentation_factor <= 1.0 or mode == \"none\":\n",
    "        return X_train, y_train\n",
    "\n",
    "    # Defaults\n",
    "    if transform_p is None:\n",
    "        transform_p = {'noise': 0.5, 'shift': 0.5, 'ripple': 0.5}\n",
    "    if transform_cfg is None:\n",
    "        transform_cfg = {\n",
    "            'noise':  {'noise_level': 0.03},\n",
    "            'shift':  {'max_shift': 2},\n",
    "            'ripple': {'amplitude': 0.01, 'freq_range': (1.0, 5.0)},\n",
    "        }\n",
    "\n",
    "    # Number of synthetic samples to add\n",
    "    n_new = int((augmentation_factor - 1.0) * N)\n",
    "    if n_new <= 0:\n",
    "        return X_train, y_train\n",
    "\n",
    "    # Sampling indices\n",
    "    if mode == \"balanced\":\n",
    "        weights = _balanced_sampling_weights(y_train, target=balanced_target)\n",
    "        chosen_idx = np.random.choice(np.arange(N), size=n_new, p=weights)\n",
    "    elif mode == \"uniform\":\n",
    "        chosen_idx = np.random.randint(0, N, size=n_new)\n",
    "    else:\n",
    "        # Shouldn't reach (caught earlier), but keep safe\n",
    "        return X_train, y_train\n",
    "\n",
    "    # Generate augmented spectra\n",
    "    aug_X = [X_train[i].copy() for i in chosen_idx]\n",
    "    aug_y = [y_train[i].copy() for i in chosen_idx]\n",
    "\n",
    "    for j in range(n_new):\n",
    "        s = aug_X[j]\n",
    "\n",
    "        if random.random() < float(transform_p.get('noise', 0.0)):\n",
    "            s = add_noise(s, **transform_cfg.get('noise', {}))\n",
    "\n",
    "        if random.random() < float(transform_p.get('shift', 0.0)):\n",
    "            s = shift_spectrum(s, **transform_cfg.get('shift', {}))\n",
    "\n",
    "        if random.random() < float(transform_p.get('ripple', 0.0)):\n",
    "            s = ripple_spectrum(s, **transform_cfg.get('ripple', {}))\n",
    "\n",
    "        aug_X[j] = s\n",
    "\n",
    "    # Concatenate and shuffle (preserve dtype)\n",
    "    X_out = np.concatenate([X_train, np.asarray(aug_X, dtype=X_train.dtype)], axis=0)\n",
    "    y_out = np.concatenate([y_train, np.asarray(aug_y, dtype=y_train.dtype)], axis=0)\n",
    "\n",
    "    # Global shuffle with same seed for determinism\n",
    "    perm = np.random.permutation(X_out.shape[0])\n",
    "    X_out = X_out[perm]\n",
    "    y_out = y_out[perm]\n",
    "    return X_out, y_out\n",
    "\n",
    "# --- Convenience wrappers (optional) ------------------------------------------\n",
    "def augment_uniform(\n",
    "    X_train: np.ndarray, y_train: np.ndarray, factor: float = 3.0, **kwargs\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    return augment_data(X_train, y_train, augmentation_factor=factor, mode=\"uniform\", **kwargs)\n",
    "\n",
    "def augment_balanced(\n",
    "    X_train: np.ndarray, y_train: np.ndarray, factor: float = 3.0,\n",
    "    balanced_target: Literal[\"mvn\",\"self-kde\"] = \"mvn\", **kwargs\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    return augment_data(\n",
    "        X_train, y_train, augmentation_factor=factor, mode=\"balanced\",\n",
    "        balanced_target=balanced_target, **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b65f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_parameter_distributions(\n",
    "    y_original: np.ndarray,\n",
    "    y_augmented: np.ndarray,\n",
    "    param_names = ('Teff', '[Fe/H]', 'log g'),\n",
    "    bins: int = 50,\n",
    "    augmented_label: str = 'After Balanced Augmentation'\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare label distributions before vs. after augmentation.\n",
    "\n",
    "    - Uses common bin edges per parameter for fair comparison\n",
    "    - Plots densities (normalized histograms)\n",
    "    - Ignores NaNs safely\n",
    "    - Returns the figure so you can save it\n",
    "    \"\"\"\n",
    "    if y_original.ndim != 2 or y_augmented.ndim != 2:\n",
    "        raise ValueError(\"y_original and y_augmented must be 2D arrays of shape (N, D).\")\n",
    "    if y_original.shape[1] != y_augmented.shape[1]:\n",
    "        raise ValueError(\"Both arrays must have the same number of columns (parameters).\")\n",
    "    if len(param_names) != y_original.shape[1]:\n",
    "        raise ValueError(\"param_names length must match number of parameters (columns).\")\n",
    "\n",
    "    D = y_original.shape[1]\n",
    "    fig, axes = plt.subplots(2, D, figsize=(5*D, 8), sharex='col')\n",
    "    fig.suptitle('Comparison of Parameter Distributions', fontsize=18)\n",
    "\n",
    "    for i in range(D):\n",
    "        yo = y_original[:, i]\n",
    "        ya = y_augmented[:, i]\n",
    "        yo = yo[np.isfinite(yo)]\n",
    "        ya = ya[np.isfinite(ya)]\n",
    "\n",
    "        # Handle degenerate cases\n",
    "        if yo.size == 0 and ya.size == 0:\n",
    "            for r in (0, 1):\n",
    "                axes[r, i].text(0.5, 0.5, 'No finite data', ha='center', va='center')\n",
    "                axes[r, i].set_axis_off()\n",
    "            continue\n",
    "\n",
    "        data_min = np.nanmin([yo.min() if yo.size else np.inf,\n",
    "                              ya.min() if ya.size else np.inf])\n",
    "        data_max = np.nanmax([yo.max() if yo.size else -np.inf,\n",
    "                              ya.max() if ya.size else -np.inf])\n",
    "        if not np.isfinite(data_min) or not np.isfinite(data_max) or data_min == data_max:\n",
    "            for r in (0, 1):\n",
    "                axes[r, i].text(0.5, 0.5, 'Insufficient spread', ha='center', va='center')\n",
    "                axes[r, i].set_axis_off()\n",
    "            continue\n",
    "\n",
    "        edges = np.linspace(data_min, data_max, bins+1)\n",
    "\n",
    "        # Row 1: original\n",
    "        axes[0, i].hist(yo, bins=edges, density=True, alpha=0.75, edgecolor='black')\n",
    "        axes[0, i].set_title(f\"{param_names[i]} - Original\", fontsize=12)\n",
    "        axes[0, i].set_ylabel('Density', fontsize=11)\n",
    "        axes[0, i].grid(True, alpha=0.25)\n",
    "\n",
    "        # Row 2: augmented\n",
    "        axes[1, i].hist(ya, bins=edges, density=True, alpha=0.75, edgecolor='black')\n",
    "        axes[1, i].set_title(f\"{param_names[i]} - {augmented_label}\", fontsize=12)\n",
    "        axes[1, i].set_xlabel(param_names[i], fontsize=11)\n",
    "        axes[1, i].set_ylabel('Density', fontsize=11)\n",
    "        axes[1, i].grid(True, alpha=0.25)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Balanced augmentation with specific transform settings\n",
    "X_train_aug_balanced, y_train_aug_balanced = augment_uniform(\n",
    "    X_train_final,\n",
    "    y_train_final,\n",
    "    factor=7.0,                          # total size multiplier\n",
    "    balanced_target=\"mvn\",               # or \"self-kde\"\n",
    "    transform_p={'noise': 0.5, 'shift': 0.5, 'ripple': 0.5},\n",
    "    transform_cfg={\n",
    "        'noise':  {'noise_level': 0.03},\n",
    "        'shift':  {'max_shift': 2},\n",
    "        'ripple': {'amplitude': 0.01, 'freq_range': (1.0, 5.0)},\n",
    "    },\n",
    "    rng_seed=42\n",
    ")\n",
    "\n",
    "# Plot (common bin edges + densities)\n",
    "fig = plot_parameter_distributions(\n",
    "    y_train_final,\n",
    "    y_train_aug_balanced,\n",
    "    param_names=['Teff', '[Fe/H]', 'log g'],\n",
    "    augmented_label='After Uniform Augmentation'\n",
    ")\n",
    "# Optionally save:\n",
    "# fig.savefig('param_dists_before_after_balanced_aug.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the parameter limits across the three splits\n",
    "print(f\"Limits in training set:\")\n",
    "print(f\"Teff: {y_train_final[:, 0].min()} - {y_train_final[:, 0].max()}\")\n",
    "print(f\"FeH:  {y_train_final[:, 1].min()} - {y_train_final[:, 1].max()}\")\n",
    "print(f\"logg: {y_train_final[:, 2].min()} - {y_train_aug_balanced[:, 2].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e5474",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dict = {\n",
    "    'teff_output': y_train_aug_balanced[:, 0],\n",
    "    'feh_output': y_train_aug_balanced[:, 1],\n",
    "    'logg_output': y_train_aug_balanced[:, 2]\n",
    "}\n",
    "\n",
    "y_test_dict = {\n",
    "    'teff_output': y_test_final[:, 0],\n",
    "    'feh_output': y_test_final[:, 1],\n",
    "    'logg_output': y_test_final[:, 2]\n",
    "}\n",
    "\n",
    "print(\"\\n--- Data shapes ---\")\n",
    "print(\"X_train shape:\", X_train_aug_balanced.shape)\n",
    "print(\"X_val_final shape:\", X_val_final.shape)\n",
    "print(\"X_test shape:\", X_test_final.shape)\n",
    "print(\"y_train_teff shape:\", y_train_dict['teff_output'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1cd42",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "We optimize a fully connected residual network (MLP-ResNet) tailored for multitask regression across the three stellar labels.\n",
    "\n",
    "- **Shared feature extractor**: stacked Dense + BatchNorm + ReLU blocks with residual skip connections capture non-linear spectral structure while keeping gradients stable.\n",
    "- **Task-specific heads**: lightweight Dense blocks with dropout and \\(L_2\\) regularization specialize the shared representation for Teff, [Fe/H], and log g predictions.\n",
    "- **Hyperparameter search**: Keras Tuner (BayesianOptimization) sweeps layer widths, activation choices, dropout rates, and optimizer configurations to match dataset scale.\n",
    "- **Training signals**: the network minimizes mean-squared error per task, monitors MAE for interpretability, and inverts the stored scalers to report metrics in physical units.\n",
    "\n",
    "This layout keeps the capacity flexible enough for precise regression while remaining lightweight for experimentation on standard GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95811b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resnet_block(x, units, dropout_rate=0.2, l2_reg=1e-4):\n",
    "    \"\"\"ResNet block composed of Dense -> BatchNorm -> ReLU -> Dropout with a skip connection.\"\"\"\n",
    "    shortcut = x\n",
    "    x = layers.Dense(\n",
    "        units,\n",
    "        kernel_regularizer=regularizers.l2(l2_reg),\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Dense(\n",
    "        units,\n",
    "        kernel_regularizer=regularizers.l2(l2_reg),\n",
    "        use_bias=False\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Adjust the shortcut connection if required\n",
    "    if shortcut.shape[-1] != units:\n",
    "        shortcut = layers.Dense(units, use_bias=False)(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_model(input_shape, num_outputs=3, dropout_rate=0.3, l2_reg=1e-4, hp=None):\n",
    "    \"\"\"\n",
    "    Multitask model built with ResNet blocks. When ``hp`` is provided, Keras Tuner samples\n",
    "    hyperparameters for the shared trunk and the task-specific heads.\n",
    "    \"\"\"\n",
    "    if isinstance(input_shape, (tuple, list)):\n",
    "        input_shape_tuple = tuple(input_shape)\n",
    "    else:\n",
    "        input_shape_tuple = (input_shape,)\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape_tuple)\n",
    "\n",
    "    if hp is None:\n",
    "        x = layers.Dense(\n",
    "            256,\n",
    "            kernel_regularizer=regularizers.l2(l2_reg),\n",
    "            use_bias=False\n",
    "        )(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "\n",
    "        x = resnet_block(x, 256, dropout_rate, l2_reg)\n",
    "        x = resnet_block(x, 128, dropout_rate, l2_reg)\n",
    "        x = resnet_block(x, 64, dropout_rate, l2_reg)\n",
    "\n",
    "        shared = layers.Dense(64, activation='relu')(x)\n",
    "    else:\n",
    "        shared_l2 = hp.Float('shared_l2', 1e-6, 1e-3, sampling='log')\n",
    "\n",
    "        initial_units = hp.Choice('initial_units', [128, 192, 256, 320])\n",
    "        x = layers.Dense(\n",
    "            initial_units,\n",
    "            kernel_regularizer=regularizers.l2(shared_l2),\n",
    "            use_bias=False\n",
    "        )(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(hp.Choice('initial_activation', ['relu', 'selu']))(x)\n",
    "        initial_dropout = hp.Float('initial_dropout', 0.0, 0.5, step=0.1)\n",
    "        if initial_dropout > 0:\n",
    "            x = layers.Dropout(initial_dropout)(x)\n",
    "\n",
    "        num_blocks = hp.Int('shared_blocks', min_value=1, max_value=3)\n",
    "        for block_idx in range(num_blocks):\n",
    "            block_units = hp.Choice(\n",
    "                f'shared_block_units_{block_idx}',\n",
    "                values=[128, 160, 192, 224, 256]\n",
    "            )\n",
    "            block_dropout = hp.Float(\n",
    "                f'shared_block_dropout_{block_idx}',\n",
    "                min_value=0.0,\n",
    "                max_value=0.5,\n",
    "                step=0.1\n",
    "            )\n",
    "            x = resnet_block(\n",
    "                x,\n",
    "                units=block_units,\n",
    "                dropout_rate=block_dropout,\n",
    "                l2_reg=shared_l2\n",
    "            )\n",
    "\n",
    "        shared_units = hp.Choice('shared_head_units', [64, 96, 128, 160, 192])\n",
    "        shared_activation = hp.Choice('shared_head_activation', ['relu', 'selu', 'gelu'])\n",
    "        shared = layers.Dense(\n",
    "            shared_units,\n",
    "            activation=shared_activation,\n",
    "            kernel_regularizer=regularizers.l2(shared_l2)\n",
    "        )(x)\n",
    "        shared_dropout = hp.Float('shared_head_dropout', 0.0, 0.5, step=0.1)\n",
    "        if shared_dropout > 0:\n",
    "            shared = layers.Dropout(shared_dropout)(shared)\n",
    "\n",
    "    outputs = []\n",
    "    for i in range(num_outputs):\n",
    "        head = shared\n",
    "        if hp is None:\n",
    "            head = layers.Dense(32, activation='relu')(head)\n",
    "            head = layers.Dropout(dropout_rate)(head)\n",
    "        else:\n",
    "            head_layers = hp.Int(f'task_{i}_layers', min_value=1, max_value=3)\n",
    "            head_units = hp.Choice(f'task_{i}_units', [32, 48, 64, 96, 128])\n",
    "            head_activation = hp.Choice(f'task_{i}_activation', ['relu', 'selu', 'gelu'])\n",
    "            head_dropout = hp.Float(f'task_{i}_dropout', 0.0, 0.5, step=0.1)\n",
    "            head_l2 = hp.Float(f'task_{i}_l2', 1e-6, 1e-3, sampling='log')\n",
    "\n",
    "            for layer_idx in range(head_layers):\n",
    "                head = layers.Dense(\n",
    "                    head_units,\n",
    "                    activation=head_activation,\n",
    "                    kernel_regularizer=regularizers.l2(head_l2)\n",
    "                )(head)\n",
    "                if head_dropout > 0:\n",
    "                    head = layers.Dropout(head_dropout)(head)\n",
    "\n",
    "        out = layers.Dense(1, activation='linear', name=f'output_{i}')(head)\n",
    "        outputs.append(out)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    if hp is None:\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=['huber'] * num_outputs,\n",
    "            metrics=['mae'] * num_outputs\n",
    "        )\n",
    "    else:\n",
    "        learning_rate = hp.Float('learning_rate', 1e-4, 5e-3, sampling='log')\n",
    "        optimizer_name = hp.Choice('optimizer', ['adam', 'nadam'])\n",
    "        if optimizer_name == 'adam':\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        else:\n",
    "            optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=['huber'] * num_outputs,\n",
    "            metrics=['mae'] * num_outputs\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_bayesian_tuner(\n",
    "    input_shape,\n",
    "    num_outputs=3,\n",
    "    max_trials=7,\n",
    "    executions_per_trial=1,\n",
    "    directory='keras_tuner',\n",
    "    project_name='stellar_multitask'\n",
    "):\n",
    "    \"\"\"Create a Bayesian tuner for the multitask model.\"\"\"\n",
    "\n",
    "    def model_builder(hp):\n",
    "        return build_model(\n",
    "            input_shape=input_shape,\n",
    "            num_outputs=num_outputs,\n",
    "            hp=hp\n",
    "        )\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        hypermodel=model_builder,\n",
    "        objective=kt.Objective('val_loss', direction='min'),\n",
    "        max_trials=max_trials,\n",
    "        executions_per_trial=executions_per_trial,\n",
    "        directory=directory,\n",
    "        project_name=project_name,\n",
    "        overwrite=False,\n",
    "    )\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_split = {\n",
    "    'output_0': y_train_dict['teff_output'],\n",
    "    'output_1': y_train_dict['feh_output'],\n",
    "    'output_2': y_train_dict['logg_output'],\n",
    "}\n",
    "y_val_split = {\n",
    "    'output_0': y_val_final[:, 0],\n",
    "    'output_1': y_val_final[:, 1],\n",
    "    'output_2': y_val_final[:, 2],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "igS0Z3KejPOP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igS0Z3KejPOP",
    "outputId": "9efbc2f7-f52c-46e7-8166-ac8e04981a77"
   },
   "outputs": [],
   "source": [
    "num_outputs = y_train_aug_balanced.shape[1]\n",
    "input_dim = X_train_aug_balanced.shape[1]\n",
    "\n",
    "print(\"\\n--- Starting Bayesian search with Keras Tuner ---\")\n",
    "tuner = build_bayesian_tuner(\n",
    "    input_shape=input_dim,\n",
    "    num_outputs=num_outputs,\n",
    "    max_trials=40,\n",
    "    executions_per_trial=1,\n",
    "    directory='keras_tuner',\n",
    "    project_name='stellar_multitask_head_search_4000_huber'\n",
    ")\n",
    "\n",
    "search_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=8, verbose=1, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "]\n",
    "\n",
    "tuner.search(\n",
    "    X_train_aug_balanced,\n",
    "    y_train_split,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val_final, y_val_split),\n",
    "    callbacks=search_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for name, value in best_hp.values.items():\n",
    "    print(f\"   {name}: {value}\")\n",
    "\n",
    "model = build_model(\n",
    "    input_shape=input_dim,\n",
    "    num_outputs=num_outputs,\n",
    "    hp=best_hp\n",
    ")\n",
    "\n",
    "final_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=12, verbose=1, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, verbose=1),\n",
    "    ModelCheckpoint('best_stellar_multitask_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"\\n--- Training the best model found ---\")\n",
    "history = model.fit(\n",
    "    X_train_aug_balanced,\n",
    "    y_train_split,\n",
    "    epochs=120,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_final, y_val_split),\n",
    "    callbacks=final_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save('stellar_regression_model.keras')\n",
    "print(\"\\nFinal model saved as 'stellar_regression_model.keras'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8694eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(history.history['output_0_mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_output_0_mae'], label='Val MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(history.history['output_1_mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_output_1_mae'], label='Val MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(history.history['output_2_mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_output_2_mae'], label='Val MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9bbccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _to_matrix(pred):\n",
    "    \"\"\"Accepts list of heads [(N,1), ...] or array (N,T); returns (N,T).\"\"\"\n",
    "    if isinstance(pred, (list, tuple)):\n",
    "        cols = []\n",
    "        for a in pred:\n",
    "            a = np.asarray(a).reshape(len(a), -1)\n",
    "            # if someone passes hetero heads [mu, log_var], keep mu\n",
    "            cols.append(a[:, 0])\n",
    "        return np.column_stack(cols)\n",
    "    A = np.asarray(pred)\n",
    "    return A if A.ndim == 2 else A.reshape(len(A), -1)\n",
    "\n",
    "def _inverse_targets(Y, scalers_y):\n",
    "    \"\"\"Inverse-transform each column using its scaler; returns (N,T).\"\"\"\n",
    "    Y = np.asarray(Y)\n",
    "    out = np.zeros_like(Y, dtype=float)\n",
    "    for i, sc in enumerate(scalers_y):\n",
    "        out[:, i] = sc.inverse_transform(Y[:, i].reshape(-1, 1)).ravel()\n",
    "    return out\n",
    "\n",
    "def bootstrap_mae(\n",
    "    model,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    scalers_y: Optional[list] = None,\n",
    "    n_bootstrap: int = 1000,\n",
    "    seed: Optional[int] = 42,\n",
    ") -> Dict[str, Tuple[float, float, Tuple[float, float]]]:\n",
    "    \"\"\"\n",
    "    Bootstrap MAE from a single trained model (no retraining).\n",
    "    Returns per-parameter (mean, sd, (ci_lo, ci_hi)) in the SAME UNITS for y_true & y_pred.\n",
    "    If `scalers_y` is provided, both y_true and y_pred are inverse-transformed to physical units.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # 1) Predict once on the full test set\n",
    "    pred = model.predict(X_test, verbose=0)\n",
    "    y_pred = _to_matrix(pred)                       # (N, T)\n",
    "    y_true = np.asarray(y_test)                     # (N, T)\n",
    "\n",
    "    # 2) Put both on the same scale (optionally inverse-transform to physical units)\n",
    "    if scalers_y is not None:\n",
    "        y_true_phys = _inverse_targets(y_true, scalers_y)\n",
    "        y_pred_phys = _inverse_targets(y_pred, scalers_y)\n",
    "    else:\n",
    "        y_true_phys = y_true\n",
    "        y_pred_phys = y_pred\n",
    "\n",
    "    N, T = y_true_phys.shape\n",
    "    boot = np.empty((n_bootstrap, T), dtype=float)\n",
    "\n",
    "    # Precompute absolute errors once\n",
    "    abs_err = np.abs(y_true_phys - y_pred_phys)     # (N, T)\n",
    "\n",
    "    # 3) Bootstrap by resampling indices\n",
    "    idx = rng.integers(0, N, size=(n_bootstrap, N))\n",
    "    # mean over rows for each bootstrap sample\n",
    "    for b in range(n_bootstrap):\n",
    "        boot[b, :] = abs_err[idx[b]].mean(axis=0)\n",
    "\n",
    "    # 4) Aggregate: mean, sd (ddof=1), and 95% percentile CI\n",
    "    stats = {}\n",
    "    for t in range(T):\n",
    "        vals = boot[:, t]\n",
    "        m = float(vals.mean())\n",
    "        s = float(vals.std(ddof=1))\n",
    "        lo, hi = np.percentile(vals, [2.5, 97.5])\n",
    "        stats[f'param_{t}'] = (m, s, (float(lo), float(hi)))\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers_y = scalers['target_scalers']  # if you want physical units\n",
    "mae_stats = bootstrap_mae(\n",
    "    model,\n",
    "    X_test_final,\n",
    "    y_test_final,     # must match the scale used during training (scaled if you trained on scaled)\n",
    "    scalers_y=scalers_y,    # set to None if you prefer scaled-units MAE\n",
    "    n_bootstrap=1000,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "for k, (m, s, (lo, hi)) in mae_stats.items():\n",
    "    print(f\"{k}: MAE = {m:} +/- {s:}  (95% CI [{lo:}, {hi:}])\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('stellar_regression_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c811b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see how many parameters the model has\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scaled_list = model.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237963f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmape(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M39U2XzPUi_d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M39U2XzPUi_d",
    "outputId": "da36b678-fee8-48ff-b37f-762a326ab544"
   },
   "outputs": [],
   "source": [
    "# Final evaluation and analysis\n",
    "\n",
    "# `predict` returns one array per output head\n",
    "predictions_scaled = np.column_stack(predictions_scaled_list)  # Shape: (n_samples, 3)\n",
    "\n",
    "param_names = ['Teff', 'Fe/H', 'log(g)']\n",
    "print(\"\\n--- Final model performance on the test set ---\")\n",
    "\n",
    "for i, name in enumerate(param_names):\n",
    "    # Select the scaled predictions for the i-th head\n",
    "    pred_scaled = predictions_scaled[:, i].reshape(-1, 1)\n",
    "\n",
    "    # Invert the scaling applied to predictions\n",
    "    pred_unscaled = scalers['target_scalers'][i].inverse_transform(pred_scaled)\n",
    "\n",
    "    # Extract the true values\n",
    "    true_scaled = y_test_final[:, i].reshape(-1, 1)\n",
    "    true_unscaled = scalers['target_scalers'][i].inverse_transform(true_scaled)\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(true_unscaled, pred_unscaled)\n",
    "    r2 = r2_score(true_unscaled, pred_unscaled)\n",
    "    wmape_value = wmape(true_unscaled, pred_unscaled)\n",
    "\n",
    "    print(f\"Parameter: {name}\")\n",
    "    print(f\"  -> Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"  -> Coefficient of Determination (R^2): {r2:.4f}\")\n",
    "    print(f\"  -> Weighted Mean Absolute Percentage Error (WMAPE): {wmape_value:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba629e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 993
    },
    "id": "5ba629e6",
    "outputId": "7ec18874-5147-4343-d75e-a761c2c84137"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Generating regression plots (no outlier filtering, no fitted line) ---\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n",
    "fig.suptitle('Regression Plots', fontsize=24)\n",
    "\n",
    "scatter_mappable = None\n",
    "\n",
    "for i, name in enumerate(param_names):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Invert the scaling for predictions and ground truth (no filtering)\n",
    "    pred_unscaled = scalers['target_scalers'][i].inverse_transform(\n",
    "        predictions_scaled_list[i].reshape(-1, 1)\n",
    "    ).flatten()\n",
    "\n",
    "    true_scaled = y_test_final[:, i].reshape(-1, 1)\n",
    "    true_values = scalers['target_scalers'][i].inverse_transform(true_scaled).flatten()\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(true_values, pred_unscaled)\n",
    "    r2 = r2_score(true_values, pred_unscaled)\n",
    "\n",
    "    # Density-colored scatter plot without any outlier filtering\n",
    "    xy = np.vstack([true_values, pred_unscaled])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    idx = z.argsort()\n",
    "    x, y, z = true_values[idx], pred_unscaled[idx], z[idx]\n",
    "    scatter_mappable = ax.scatter(x, y, c=z, s=15, cmap='viridis', alpha=0.7)\n",
    "\n",
    "    # y = x reference line\n",
    "    lim_min = np.min(np.concatenate([x, y]))\n",
    "    lim_max = np.max(np.concatenate([x, y]))\n",
    "    lims = [lim_min, lim_max]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.8, zorder=3, label='Ideal')\n",
    "\n",
    "    # >>> Removed: linear regression fit <<<\n",
    "    # m, b = np.polyfit(true_values, pred_unscaled, 1)\n",
    "    # ax.plot(np.array(lims), m * np.array(lims) + b, color='orange', linestyle='-',\n",
    "    #         linewidth=2, zorder=2, label='Regression')\n",
    "\n",
    "    # Text box with summary metrics\n",
    "    stats_text = f'MAE = {mae:.4f}\\n$R^2$ = {r2:.4f}'\n",
    "    ax.text(0.05, 0.95, stats_text,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=16, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='square,pad=0.5', fc='wheat', alpha=0.5))\n",
    "\n",
    "    ax.set_title(f'Parameter: {name}', fontsize=18)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.legend(fontsize=14)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Predicted', fontsize=16)\n",
    "    ax.set_xlabel('True Value', fontsize=16)\n",
    "\n",
    "# Layout and color bar\n",
    "fig.tight_layout(rect=[0, 0, 0.9, 0.95])\n",
    "if scatter_mappable is not None:\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    cbar = fig.colorbar(scatter_mappable, cax=cbar_ax)\n",
    "    cbar.set_label('Point Density', rotation=270, labelpad=25, fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save a and b from regression lines for later use\n",
    "regression_params = {}\n",
    "for i, name in enumerate(param_names):\n",
    "    # Invert the scaling for predictions and ground truth (no filtering)\n",
    "    pred_unscaled = scalers['target_scalers'][i].inverse_transform(\n",
    "        predictions_scaled_list[i].reshape(-1, 1)\n",
    "    ).flatten()\n",
    "\n",
    "    true_scaled = y_test_final[:, i].reshape(-1, 1)\n",
    "    true_values = scalers['target_scalers'][i].inverse_transform(true_scaled).flatten()\n",
    "\n",
    "    # Linear regression fit\n",
    "    m, b = np.polyfit(true_values, pred_unscaled, 1)\n",
    "    regression_params[name] = (m, b)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
