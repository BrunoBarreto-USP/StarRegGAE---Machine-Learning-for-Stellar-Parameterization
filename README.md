# StellarRegGAE
<img width="256" height="256" alt="image" src="https://github.com/user-attachments/assets/c3fb3c42-fa61-44d1-ac82-d24e13e50008" />

**Stellar Parameter Regression with MLPâ€‘ResNet (Multitask)**

A clean, reproducible pipeline for multitask regression of stellar atmospheric parameters (Teff, [Fe/H], log g) from mediumâ€‘resolution optical spectra. It uses a compact MLPâ€‘ResNet backbone with taskâ€‘specific heads, strong preprocessing/augmentation, and Bayesian hyperparameter tuning.

> Notebook: `StellarRegGAE.ipynb` (converted into a repo layout below).

## ðŸš€ Key Features

* **Endâ€‘toâ€‘end pipeline**: loading â†’ cleaning â†’ normalization â†’ augmentation â†’ training â†’ evaluation â†’ persistence.
* **Multitask head**: predicts **Teff (K)**, **[Fe/H] (dex)**, and **log g (dex)** jointly from the same spectrum.
* **Compact MLPâ€‘ResNet**: stacked Dense blocks with residual/skip connections, BatchNorm, Dropout.
* **Bayesian hyperâ€‘tuning**: search over widths/depths/dropout/regularization/optimizer params with Keras Tuner.
* **Robust preprocessing**: IQRâ€‘based filters, continuumâ€‘normalized fluxes, label scaling, duplicate handling.
* **Data augmentation**: Gaussian noise, small wavelength shifts, lowâ€‘frequency â€œrippleâ€ continuum perturbations.
* **Transparent metrics**: MAE and RÂ² in **physical units** via inverseâ€‘transformed scalers.
* **Reproducibility**: seeds set; models and scalers saved to disk.

## ðŸ“¦ Data

* `X_raw_resampled_filtered_train.npy` â€” continuumâ€‘normalized flux arrays (train)
* `y_raw_loaded_filtered_train.npy` â€” labels `[Teff, [Fe/H], log g]` (train)
* `X_raw_resampled_filtered_test.npy` â€” flux arrays (test)
* `y_raw_loaded_filtered_test.npy` â€” labels (test)

> Each **row** is a spectrum; each **column** is a wavelength bin.

If your file names differ, edit the respective paths in the notebook cells.

## ðŸ§° Requirements

Create a `requirements.txt` with:

```
joblib
keras
keras-tuner
matplotlib
numpy
gdown
scipy
scikit-learn
tensorflow>=2.12
```

Python â‰¥ 3.9 is recommended. TensorFlow should target your local CUDA/cuDNN stack if using GPU.

## ðŸ§ª Model & Training Overview

* **Backbone**: MLPâ€‘ResNet (Dense â†’ BN â†’ ReLU â†’ Dropout) Ã— N with skip connections.
* **Heads**: three lightweight Dense branches for Teff, [Fe/H], log g.
* **Loss**: perâ€‘task **MSE** (sum across heads); report **MAE** and **RÂ²**.
* **Tuning**: Keras Tuner (BayesianOptimization) over widths, depth, dropout, L2, learning rate, etc.
* **Callbacks**: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint (best by val loss).
* **Saved artifacts**: model (`stellar_regression_model.keras`).

## ðŸ“ˆ Results & Diagnostics

The notebook includes:

* Distribution plots for labels and quality filters.
* Train/validation curves and perâ€‘task MAE histories.
* Predicted vs. true scatter with 1:1 reference.

## ðŸ§© Extending

* Swap in different augmentations (telluric masks, stronger continuum ripple).
* Add physicsâ€‘inspired priors or wavelengthâ€‘attention masks per parameter.
* Try Conv1D or spectral transformers for local pattern sensitivity.
* Convert to TF SavedModel / ONNX for deployment.
**Q: How do I reproduce the exact hyperâ€‘parameters?**
A: The notebook seeds and search spaces are fixed; the best config is saved via `ModelCheckpoint` and can be reloaded from `models/`.


<img width="2126" height="592" alt="image" src="https://github.com/user-attachments/assets/f76a4850-1f4e-48e5-97bf-7b813f12134c" />
